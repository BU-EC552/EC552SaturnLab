{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "399aa001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a topic modeling script for synth bio papers on PMC\n",
    "# this is a test notebook file\n",
    "# this is an example query program\n",
    "# code taken from https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n",
    "# Cred. Susan Li\n",
    "from Bio import Entrez\n",
    "import json\n",
    "\n",
    "def search(query):\n",
    "    Entrez.email = 'clp0216@bu.edu'\n",
    "    handle = Entrez.esearch(db='pmc',\n",
    "                            sort='relevance',\n",
    "                            retmax='350',\n",
    "                            retmode='xml',\n",
    "                            term=query)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "def fetch_details(id_list):\n",
    "    ids = ','.join(id_list)\n",
    "    Entrez.email = 'your.email@example.com'\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids)\n",
    "    results = Entrez.read(handle)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ff7b91",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "329\n"
     ]
    }
   ],
   "source": [
    "results1 = search('ACS Synth. Biol[Journal]')\n",
    "ID1 = results1['IdList']\n",
    "print(len(ID1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6770ec",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "196\n"
     ]
    }
   ],
   "source": [
    "results2 = search('Synth Syst Biotechnol[Journal]')\n",
    "ID2 = results2['IdList']\n",
    "print(len(ID2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "114950a0",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "350\n"
     ]
    }
   ],
   "source": [
    "results3 = search('Front Mol Neurosci.[Journal]')\n",
    "ID3 = results3['IdList']\n",
    "print(len(ID3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f641f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_list = ID1+ID2+ID3\n",
    "id_list = id_list[:350]\n",
    "# papers = fetch_details(id_list)\n",
    "# for i, paper in enumerate(papers['PubmedArticle']):\n",
    "#      print(\"{}) {}\".format(i+1, paper['MedlineCitation']['Article']['ArticleTitle']))\n",
    "\n",
    "\n",
    "# train on curated models based upon selected journals idenified in bibliometric searches:\n",
    "# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5533824/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cb5f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import xmltodict\n",
    "import requests\n",
    "payload = {'db':'pmc', 'id':id_list}\n",
    "# response = requests.get(\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4304705\")\n",
    "response = requests.get(\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\", params=payload)\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import xmltodict\n",
    "from json_flatten import flatten\n",
    "    \n",
    "string_xml = response.text\n",
    "tree = ET.fromstring(string_xml)\n",
    "xml_str = ET.tostring(tree, method='xml')\n",
    "data = xmltodict.parse(xml_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "169faf1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0045d662",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(data['pmc-articleset']['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508e76cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(filter(lambda x:, data['pmc-articleset']['article']['body']['sec'][2]['sec'][1]))\n",
    "# https://towardsdatascience.com/how-to-flatten-deeply-nested-json-objects-in-non-recursive-elegant-python-55f96533103d\n",
    "from collections import MutableMapping\n",
    "import numpy as np\n",
    "from json_flatten import flatten\n",
    "import os, glob\n",
    "# for filename in glob.glob(os.getcwd() + \"/text_result_*\"):\n",
    "#     os.remove(filename) \n",
    "\n",
    "kwds = {}\n",
    "original_corpus = {}\n",
    "# code to convert ini_dict to flattened dictionary\n",
    "for i, paper in enumerate(data['pmc-articleset']['article']):\n",
    "    try:\n",
    "        pmid = paper['front']['article-meta']['article-id'][0][\"#text\"]\n",
    "        flat_data = flatten(paper['body'])\n",
    "        # apply filter for any value over 10 characters in length\n",
    "        text_res = dict((key, flat_data[key]) for key in flat_data.keys() if (len(flat_data[key]) > 30) and ('{' not in flat_data[key]))\n",
    "        concat_text = \" \".join(list(text_res.values())) + \".\"\n",
    "        concat_text = concat_text.replace(\"[]\", \"\").replace(\"[,]\", \"\")\n",
    "        original_corpus[pmid] = concat_text\n",
    "        kwds[pmid] = paper['front']['article-meta']['kwd-group']['kwd']\n",
    "    except:\n",
    "        pass\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "109801c0",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "len(original_corpus.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import chdir\n",
    "os.chdir('/Users/cullenpaulisick/Documents/EC552/Project/EC552SaturnLab/PracticeCorpus')\n",
    "for item in original_corpus.items():\n",
    "    with open('pmid_' + str(item[0]) + '.txt', 'w') as f:\n",
    "        f.write(item[1])\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e329bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bow_corpus(original_corpus):\n",
    "    docs = list(original_corpus)\n",
    "    # Tokenize the documents.\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    # Split the documents into tokens.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 2] for doc in docs]\n",
    "\n",
    "    # Lemmatize the documents.\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "    \n",
    "    # Compute bigrams.\n",
    "    from gensim.models import Phrases\n",
    "\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "    bigram = Phrases(docs, min_count=20)\n",
    "    for idx in range(len(docs)):\n",
    "        for token in bigram[docs[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                docs[idx].append(token)\n",
    "\n",
    "    # Remove rare and common tokens.\n",
    "    from gensim.corpora import Dictionary\n",
    "\n",
    "    # Create a dictionary representation of the documents.\n",
    "    dictionary = Dictionary(docs)\n",
    "\n",
    "    # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "    dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "    print(docs[0][:5])\n",
    "    # Bag-of-words representation of the documents.\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    print('Number of unique tokens: %d' % len(dictionary))\n",
    "    print('Number of documents: %d' % len(corpus))\n",
    "    return corpus, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71e3fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_string(doc):\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    from nltk.corpus import stopwords\n",
    "    from gensim.corpora import Dictionary\n",
    "    import numpy as np\n",
    "    doc = doc.lower()\n",
    "    print(doc)\n",
    "    print(\"Doc: \")\n",
    "    print(doc)\n",
    "    # tokenize string\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    doc = doc.lower()  # Convert to lowercase.\n",
    "    doc = tokenizer.tokenize(doc)  # Split into words.\n",
    "    print(\"Doc after tokenization: \")\n",
    "    print(doc)\n",
    "    # remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    user_terms = [token for token in doc if token not in stop_words]\n",
    "    print(\"Doc after stopword filtering: \")\n",
    "    print(user_terms)\n",
    "    # convert terms into word-order agnostic 2D list\n",
    "    user_array = np.array(user_terms)\n",
    "    user_corpus = [np.roll(user_array, i) for i in range(len(user_terms))]\n",
    "    user_corpus = [list(arr) for arr in user_corpus]\n",
    "    # create dictionary and bag-of-words\n",
    "    dictionary = Dictionary(user_corpus)\n",
    "    print(user_corpus)\n",
    "    text_corpus = [dictionary.doc2bow(doc) for doc in user_corpus]\n",
    "    print(\"Corpus created from text: \")\n",
    "    print(text_corpus)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ad73e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'Hate', 'the', 'Ravens'], ['Ravens', 'I', 'Hate', 'the'], ['the', 'Ravens', 'I', 'Hate'], ['Hate', 'the', 'Ravens', 'I']]\n"
     ]
    }
   ],
   "source": [
    "user_terms = ['I', 'Hate', 'the', 'Ravens']\n",
    "user_array = np.array(user_terms)\n",
    "user_corpus = [np.roll(user_array, i) for i in range(len(user_terms))]\n",
    "user_corpus = [list(arr) for arr in user_corpus]\n",
    "print(user_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe579f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cell', 'free', 'system', 'that', 'support']\n",
      "Number of unique tokens: 8938\n",
      "Number of documents: 136\n",
      "['central', 'goal', 'synthetic', 'biology', 'build']\n",
      "Number of unique tokens: 4546\n",
      "Number of documents: 45\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "length_corpus = len(list(original_corpus.values()))\n",
    "train_cutoff = math.ceil(length_corpus*0.75)\n",
    "train_kwds = list(kwds.values())[:train_cutoff]\n",
    "train_kwds_id = list(kwds.keys())[:train_cutoff]\n",
    "test_kwds = list(kwds.values())[train_cutoff:]\n",
    "test_kwds_id = list(kwds.keys())[train_cutoff:]\n",
    "train_data = list(original_corpus.values())[:train_cutoff]\n",
    "test_data = list(original_corpus.values())[train_cutoff:]\n",
    "training = bow_corpus(train_data)\n",
    "testing = bow_corpus(test_data)\n",
    "\n",
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "train_corpus = training[0]\n",
    "test_corpus = testing[0]\n",
    "dictionary = training[1]\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 500\n",
    "passes = 15\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=train_corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07bf2b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.ldamodel.LdaModel"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd04378d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pveg promoter and long time optimization\n",
      "Doc: \n",
      "pveg promoter and long time optimization\n",
      "Doc after tokenization: \n",
      "['pveg', 'promoter', 'and', 'long', 'time', 'optimization']\n",
      "Doc after stopword filtering: \n",
      "['pveg', 'promoter', 'long', 'time', 'optimization']\n",
      "[['pveg', 'promoter', 'long', 'time', 'optimization'], ['optimization', 'pveg', 'promoter', 'long', 'time'], ['time', 'optimization', 'pveg', 'promoter', 'long'], ['long', 'time', 'optimization', 'pveg', 'promoter'], ['promoter', 'long', 'time', 'optimization', 'pveg']]\n",
      "Corpus created from text: \n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bow_string(\"pveg promoter and long time optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "30ace3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "model.save('model1.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8d996366",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = LdaModel.load('model1.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "431e01ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8cf95aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -4.0540.\n",
      "[([(0.016739553, 'library'),\n",
      "   (0.0111876605, 'rb'),\n",
      "   (0.007410834, 'biosensor'),\n",
      "   (0.007115362, 'figure'),\n",
      "   (0.006646982, 'oscillation'),\n",
      "   (0.0065360283, 'repressor'),\n",
      "   (0.0065265405, 'activator'),\n",
      "   (0.006102303, 'circuit'),\n",
      "   (0.005772312, 'feedback'),\n",
      "   (0.005644147, 'riboswitch'),\n",
      "   (0.0056192447, 'parameter'),\n",
      "   (0.0047752857, 'induction'),\n",
      "   (0.0047452287, 'iptg'),\n",
      "   (0.0046593146, 'performance'),\n",
      "   (0.0046215598, 'screening'),\n",
      "   (0.0045057926, 'loop'),\n",
      "   (0.0037863704, 'primary'),\n",
      "   (0.0037785026, 'space'),\n",
      "   (0.0037740006, 'facs'),\n",
      "   (0.0033366706, 'strength')],\n",
      "  -1.3579791546389646),\n",
      " ([(0.0066813813, 'glucose'),\n",
      "   (0.006120781, 'riboswitch'),\n",
      "   (0.0051763025, 'colony'),\n",
      "   (0.0050295894, 'metabolic'),\n",
      "   (0.00462217, 'module'),\n",
      "   (0.004382617, 'genome'),\n",
      "   (0.004258925, 'violacein'),\n",
      "   (0.0039314055, 'yeast'),\n",
      "   (0.0035941917, 'cultivation'),\n",
      "   (0.0035560613, 'primer'),\n",
      "   (0.0034598943, 'fragment'),\n",
      "   (0.0032999702, 'cassette'),\n",
      "   (0.0030941414, 'mutant'),\n",
      "   (0.0030404662, 'codon'),\n",
      "   (0.003027584, 'thiamine'),\n",
      "   (0.0030194186, 'compound'),\n",
      "   (0.0029758061, 'day'),\n",
      "   (0.0028824531, 'aptamer'),\n",
      "   (0.0028420647, 'transformed'),\n",
      "   (0.002764774, 'editing')],\n",
      "  -1.5559875198079638),\n",
      " ([(0.008204979, 'amino'),\n",
      "   (0.007843996, 'receptor'),\n",
      "   (0.007674699, 'peptide'),\n",
      "   (0.0069032954, 'amino_acid'),\n",
      "   (0.0066378526, 'substrate'),\n",
      "   (0.005902592, 'myod'),\n",
      "   (0.0050257505, 'ligand'),\n",
      "   (0.004805176, 'residue'),\n",
      "   (0.004549373, 'mutation'),\n",
      "   (0.004412488, 'fusion'),\n",
      "   (0.003924714, 'zinc'),\n",
      "   (0.003511057, 'mammalian'),\n",
      "   (0.0031315724, 'secretion'),\n",
      "   (0.0030571432, 'figure'),\n",
      "   (0.0029901234, 'serum'),\n",
      "   (0.0029455172, 'library'),\n",
      "   (0.00290547, 'gfp'),\n",
      "   (0.002863402, 'position'),\n",
      "   (0.0028110014, 'pcb'),\n",
      "   (0.0027272657, 'light')],\n",
      "  -1.7945575932299884),\n",
      " ([(0.006171216, 'genome'),\n",
      "   (0.005209804, 'core'),\n",
      "   (0.0040255184, 'module'),\n",
      "   (0.0038777043, 'editing'),\n",
      "   (0.003842387, 'curve'),\n",
      "   (0.003803372, 'mda'),\n",
      "   (0.0037599911, 'egfp'),\n",
      "   (0.003682769, 'light'),\n",
      "   (0.003527374, 'cas9'),\n",
      "   (0.0034440432, 'group'),\n",
      "   (0.0032366365, 'crispr'),\n",
      "   (0.0031272955, 'gfp'),\n",
      "   (0.003106569, 'mutation'),\n",
      "   (0.0030621917, 'integration'),\n",
      "   (0.0030549392, 'background'),\n",
      "   (0.0029586102, 'response_curve'),\n",
      "   (0.002901962, 'regeneration'),\n",
      "   (0.0028619217, 'reporter'),\n",
      "   (0.0028341918, 'protease'),\n",
      "   (0.0027714067, 'drug')],\n",
      "  -2.7137366169261945),\n",
      " ([(0.009529686, 'noise'),\n",
      "   (0.0069678845, 'peptide'),\n",
      "   (0.006760276, 'network'),\n",
      "   (0.0067121, 'base'),\n",
      "   (0.0057019736, 'oscillator'),\n",
      "   (0.00517349, 'strand'),\n",
      "   (0.004768343, 'fuel'),\n",
      "   (0.00460834, 'ribosome'),\n",
      "   (0.0044928496, 'fragment'),\n",
      "   (0.0044677886, 'parameter'),\n",
      "   (0.004319529, 'degradation'),\n",
      "   (0.004276274, 'pg10'),\n",
      "   (0.004185748, 'mrna'),\n",
      "   (0.0039679757, 'leakage'),\n",
      "   (0.003733549, 'yeast'),\n",
      "   (0.003370556, 'fragmented'),\n",
      "   (0.003277772, 'distribution'),\n",
      "   (0.0032127986, 'pair'),\n",
      "   (0.0029900027, 'gfp'),\n",
      "   (0.0029384152, 'availability')],\n",
      "  -3.338895573091797),\n",
      " ([(0.010788162, 'circuit'),\n",
      "   (0.00880743, 'reporter'),\n",
      "   (0.0064547206, 'hac'),\n",
      "   (0.005556508, 'inducible'),\n",
      "   (0.0047922707, 'inducer'),\n",
      "   (0.0044615236, 'alphoid'),\n",
      "   (0.0043923794, 'cassette'),\n",
      "   (0.0043075876, 'tetr'),\n",
      "   (0.0042449236, 'induction'),\n",
      "   (0.0041162376, 'lexa'),\n",
      "   (0.0038469327, 'gene_circuit'),\n",
      "   (0.0037360755, 'biosensors'),\n",
      "   (0.0036287438, 'marker'),\n",
      "   (0.0036230825, 'dapg'),\n",
      "   (0.0036205065, 'copy'),\n",
      "   (0.003465289, 'platform'),\n",
      "   (0.0033978964, 'so'),\n",
      "   (0.0033722648, 'dox'),\n",
      "   (0.0033466616, 'across'),\n",
      "   (0.0032993257, 'figure')],\n",
      "  -4.997259889034492),\n",
      " ([(0.02149295, 'light'),\n",
      "   (0.0072043184, 'membrane'),\n",
      "   (0.0057302867, 'figure'),\n",
      "   (0.0050400496, 'lipid'),\n",
      "   (0.004042558, 'node'),\n",
      "   (0.003904006, 'dark'),\n",
      "   (0.003762298, 'gmp'),\n",
      "   (0.003492711, 'scaffold'),\n",
      "   (0.003423653, 'phyb'),\n",
      "   (0.0033076701, 'protease'),\n",
      "   (0.003165099, 'methanol'),\n",
      "   (0.0028782296, 'illumination'),\n",
      "   (0.0028526315, 'liposome'),\n",
      "   (0.0028485786, 'far'),\n",
      "   (0.002818375, 'phospholipid'),\n",
      "   (0.002795565, 'peptide'),\n",
      "   (0.0027793287, 'input'),\n",
      "   (0.0026671072, 'vesicle'),\n",
      "   (0.0025897436, 'far_red'),\n",
      "   (0.0025353287, 'diffusion')],\n",
      "  -5.536708568829451),\n",
      " ([(0.0106625855, 'switch'),\n",
      "   (0.0063366657, 'material'),\n",
      "   (0.0062976503, 'car'),\n",
      "   (0.005454649, 'peptide'),\n",
      "   (0.005269033, 'nisb'),\n",
      "   (0.0049582445, 'bacteria'),\n",
      "   (0.004581264, 'hydrogel'),\n",
      "   (0.0045375316, 'circuit'),\n",
      "   (0.004462397, 'nisin'),\n",
      "   (0.0044306987, 'classifier'),\n",
      "   (0.004338717, 'figure'),\n",
      "   (0.004070635, 'surface'),\n",
      "   (0.004046612, 'day'),\n",
      "   (0.003652851, 'his'),\n",
      "   (0.0032974686, 'printed'),\n",
      "   (0.0031048337, 'printing'),\n",
      "   (0.0029912875, 'mrsa'),\n",
      "   (0.0028684027, 'population'),\n",
      "   (0.0028564243, 'performance'),\n",
      "   (0.0028163604, 'class')],\n",
      "  -5.949170534459703),\n",
      " ([(0.020417767, 'hac'),\n",
      "   (0.011741931, 'array'),\n",
      "   (0.010726081, 'alphoid'),\n",
      "   (0.008395784, 'cenp'),\n",
      "   (0.008122344, 'glycosylation'),\n",
      "   (0.007781384, 'α21'),\n",
      "   (0.006701012, 'alphoid_hac'),\n",
      "   (0.0066459025, 'human'),\n",
      "   (0.0058801733, 'circuit'),\n",
      "   (0.005371894, 'chromosome'),\n",
      "   (0.004899012, 'linked'),\n",
      "   (0.0047987797, 'teto'),\n",
      "   (0.0046433257, 'nfat'),\n",
      "   (0.0045300294, 'copy'),\n",
      "   (0.004477346, 'centromere'),\n",
      "   (0.004458185, 'clone'),\n",
      "   (0.004166069, 'hacs'),\n",
      "   (0.00396674, 'chromatin'),\n",
      "   (0.00347687, 'therapeutic'),\n",
      "   (0.0033866337, 'glycans')],\n",
      "  -6.325363964123043),\n",
      " ([(0.008505148, 'mrna'),\n",
      "   (0.0071653947, 'input'),\n",
      "   (0.0066548367, 'cov'),\n",
      "   (0.0060186787, 'hif'),\n",
      "   (0.005806288, 'primer'),\n",
      "   (0.005690533, 'output'),\n",
      "   (0.005546022, 'detection'),\n",
      "   (0.005498789, 'sars'),\n",
      "   (0.005374922, 'editing'),\n",
      "   (0.005276446, 'sars_cov'),\n",
      "   (0.004923798, 'amplification'),\n",
      "   (0.004598632, 'circuit'),\n",
      "   (0.0045173652, 'figure'),\n",
      "   (0.0040909643, 'polymerase'),\n",
      "   (0.0040866155, 'strand'),\n",
      "   (0.0040555247, 'device'),\n",
      "   (0.0037427268, 'cone'),\n",
      "   (0.0034585046, 'logic'),\n",
      "   (0.003259844, 'yfp'),\n",
      "   (0.0031657498, 'cfp')],\n",
      "  -6.9700791603051)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(train_corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16284507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.07618819), (2, 0.017732281), (3, 0.22501023), (4, 0.48947626), (5, 0.07544329), (6, 0.040534757), (7, 0.058275856), (9, 0.013862014)]\n",
      "['protein−protein interaction', 'synthetic biology', 'coiled coil', 'heterodimer']\n",
      "22558529\n"
     ]
    }
   ],
   "source": [
    "print(model[test_corpus[3]])\n",
    "print(test_kwds[3])\n",
    "print(test_kwds_id[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b691e8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"genome\" + 0.007*\"primer\" + 0.007*\"fragment\" + 0.006*\"editing\" + 0.006*\"library\"'),\n",
       " (1,\n",
       "  '0.007*\"riboswitch\" + 0.005*\"mutation\" + 0.004*\"library\" + 0.003*\"iptg\" + 0.003*\"lipid\"'),\n",
       " (2,\n",
       "  '0.006*\"module\" + 0.005*\"biosensor\" + 0.005*\"base\" + 0.005*\"glucose\" + 0.005*\"metabolic\"'),\n",
       " (3,\n",
       "  '0.011*\"core\" + 0.010*\"circuit\" + 0.008*\"copy\" + 0.007*\"cassette\" + 0.006*\"reporter\"'),\n",
       " (4,\n",
       "  '0.015*\"circuit\" + 0.012*\"figure\" + 0.009*\"repressor\" + 0.008*\"activator\" + 0.008*\"oscillation\"'),\n",
       " (5,\n",
       "  '0.017*\"light\" + 0.016*\"peptide\" + 0.008*\"substrate\" + 0.008*\"amino\" + 0.008*\"amino_acid\"'),\n",
       " (6,\n",
       "  '0.029*\"hac\" + 0.016*\"alphoid\" + 0.013*\"array\" + 0.010*\"alphoid_hac\" + 0.009*\"cenp\"'),\n",
       " (7,\n",
       "  '0.012*\"light\" + 0.012*\"noise\" + 0.007*\"parameter\" + 0.007*\"figure\" + 0.005*\"distribution\"'),\n",
       " (8,\n",
       "  '0.007*\"glycosylation\" + 0.004*\"bacteria\" + 0.004*\"linked\" + 0.004*\"yeast\" + 0.004*\"ligand\"'),\n",
       " (9,\n",
       "  '0.011*\"switch\" + 0.008*\"figure\" + 0.006*\"car\" + 0.006*\"drug\" + 0.006*\"light\"')]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a98e85de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(0.007511505, 'activation'),\n",
      "  (0.007097461, 'switch'),\n",
      "  (0.004782665, 'protease'),\n",
      "  (0.0047546686, 'reporter'),\n",
      "  (0.0046180845, 'core'),\n",
      "  (0.004326322, 'peptide'),\n",
      "  (0.0040891776, 'circuit'),\n",
      "  (0.003087302, 'gfp'),\n",
      "  (0.0029444091, 'figure'),\n",
      "  (0.0028294383, 'ligand'),\n",
      "  (0.002761806, 'fusion'),\n",
      "  (0.0027580173, 'cleavage'),\n",
      "  (0.0026285388, 'output'),\n",
      "  (0.0024671573, 'car'),\n",
      "  (0.0024269305, 'core_promoter'),\n",
      "  (0.0024224874, 'nfat'),\n",
      "  (0.00233404, 'input'),\n",
      "  (0.0023046061, 'scaffold'),\n",
      "  (0.0022477582, 'primer'),\n",
      "  (0.002186634, 'secretion')],\n",
      " -2.980139296128298)\n"
     ]
    }
   ],
   "source": [
    "pprint(top_topics[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19702c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ATAC-seq', 'heterologous gene', 'RNA-seq', 'locus', 'genome\\nengineering']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(kwds.values())[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ec22472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9e5bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "user_terms = \"I hope that the pVeg system functions as a promoter\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "user_terms = user_terms.lower()  # Convert to lowercase.\n",
    "user_terms = tokenizer.tokenize(user_terms)  # Split into words.\n",
    "stop_words = stopwords.words('english')\n",
    "user_terms = [word for word in user_terms if word not in stop_words ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83b4b2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hope', 'pveg', 'system', 'functions', 'promoter']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc470c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('EC552Proj': conda)",
   "metadata": {
    "interpreter": {
     "hash": "69869beadf32d8499e4cf71b9973518647042b0f5cab7f34319854baa9d1be91"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}